{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interest in neural networks, and in particular those with architechures that support deep learning, has been surging in recent years.\n",
    "\n",
    "In this notebook we will be revisiting the problem of digit classification on the MNIST data. In doing so, we will introduce a new Python library, Tensorflow, for working with neural networks. \n",
    "\n",
    "Some words to TensorFlow.\n",
    "\n",
    "In part 1, we'll introduce Tensorflow, and refresh ourselves on the MNIST dataset. In part 2, we'll create a multi-layer neural network with a simple architechure, and train it using backpropagation. Part 3 will introduce the convolutional architechure, which can be said to be doing 'deep learning' (also called feature learning or representation learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Basics\n",
    "\n",
    "Lets start to look at Tensorflow. If later you'd like to go deeper into Tensorflow, you may want to read these tutorials: \n",
    "https://www.tensorflow.org/tutorials/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from IPython.display import display, clear_output \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic Tensorflow addition\n",
    "tf.reset_default_graph()\n",
    "# We build a graph by defining constants and operations (and later variables and placeholders). \n",
    "# By tf.constant(), a TensorFlow object is generated from a python object.\n",
    "a = tf.constant(1.0, shape=[], dtype=tf.float16)\n",
    "b = tf.constant(2.0, shape=[], dtype=tf.float16)\n",
    "add1 = tf.add(a,b)\n",
    "add2 = a + b\n",
    "\n",
    "# To execute the graph we need to start a session. \n",
    "with tf.Session() as sess:\n",
    "    result = sess.run([add1])\n",
    "    print(result[0])\n",
    "    result = sess.run([add2])\n",
    "    print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "B = tf.constant([0,1,1,1], shape=[2,2])\n",
    "\n",
    "fibonacci_init = tf.constant([[0],[1]])\n",
    "fibonacci = tf.get_variable('fibonacci', initializer=fibonacci_init)\n",
    "\n",
    "# Variable can be assigned and changed. If we run step in a session, fibonacci is overwriten by B*fibonacci\n",
    "# Matmul stands for matrix multiplication.\n",
    "\n",
    "#fibonacci = tf.matmul(B, fibonacci)  \n",
    "fibonacci_step = fibonacci.assign(tf.matmul(B, fibonacci))\n",
    "\n",
    "print(fibonacci)\n",
    "with tf.Session() as sess:\n",
    "    # Variables need to be initialized.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(8):\n",
    "        # We can run several operation simultaniously. The order of execution is random.\n",
    "\n",
    "        res_fibonacci, _ = sess.run([fibonacci, fibonacci_step])\n",
    "        print('Step: ', i, ' Next fibonacci number: ', res_fibonacci[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Tensorflow, we build a graph by defining tensors.\n",
    "# If we run a cell twice without the command tf.reset_default_grpah() all tensors are doubly defined, \n",
    "# which can lead to errors.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.get_variable('x', initializer=1.0)\n",
    "\n",
    "x = tf.Print(x, [x], 'x is calculated.')\n",
    "y = x + 2.0\n",
    "y = tf.Print(y, [y], 'y is calculated.')\n",
    "z = y - 4\n",
    "z = tf.Print(z, [z], 'z is calculated.')\n",
    "v = tf.multiply(x,3)\n",
    "v = tf.Print(v, [v], 'v is calculated.')\n",
    "\n",
    "# Now out graph is defined. In the session we will execute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency of the graph:\n",
    "\n",
    "\\begin{align*}\n",
    " &x \\\\\n",
    "\\swarrow & \\searrow  \\\\\n",
    "v \\quad & \\quad y \\\\\n",
    " & \\ \\quad \\searrow\\\\\n",
    " & \\ \\qquad z\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # In the session ONLY the nodes of the graph will be calculated, which are needed\n",
    "    # for the arguments in sess.run()\n",
    "    # The TF print statement, prints into the terminal. Have a look which variables are calculated\n",
    "    # by running this session.\n",
    "    res = sess.run([v])[0]\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train_data and train_labels\n",
    "target_beta = 4.0\n",
    "noise = 0.2\n",
    "train_data = np.linspace(-1, 1, 100)\n",
    "train_labels = target_beta * train_data + np.random.rand(train_data.shape[0]) * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "trainX = tf.constant(train_data, dtype = tf.float32)\n",
    "trainY = tf.constant(train_labels, dtype = tf.float32)\n",
    "learningRate = tf.constant(0.1)\n",
    "# without initializer, variables are initializes with random values between -1 and 1.\n",
    "beta = tf.get_variable('beta', shape=[1])\n",
    "\n",
    "def model(this_beta, x):\n",
    "    return tf.multiply(this_beta, x)\n",
    "\n",
    "def cost(this_beta):\n",
    "    return tf.reduce_mean((model(this_beta, trainX) - trainY)**2) / 2\n",
    "\n",
    "cc = cost(beta)\n",
    "\n",
    "## Gradient descent by hand\n",
    "#def grad(this_beta):\n",
    "#    return tf.reduce_mean(tf.multiply(model(this_beta, trainX) - trainY, trainX))\n",
    "#step = beta.assign(beta - learningRate * grad(beta))\n",
    "\n",
    "## Gradient descent by tensorflow\n",
    "gd = tf.train.GradientDescentOptimizer(learningRate)\n",
    "step = gd.minimize(cc)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    cost_vec = []\n",
    "    # By running step multiple times, we perform gradient descent for the variable beta. \n",
    "    # For each step, we can ouput the cost function.\n",
    "    for i in range(150):\n",
    "        _, cost = sess.run([step, cc])\n",
    "        cost_vec.append(cost)\n",
    "    plt.plot(cost_vec)\n",
    "    my_beta = sess.run([beta])[0]\n",
    "    print(my_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we add an intercept to the model above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPLEMENT: try adding an intercept to the above model\n",
    "# Creating train_data and train_labels\n",
    "target_beta_1 = 4.0\n",
    "target_beta_0 = -2.0\n",
    "noise = 0.2\n",
    "train_data = np.linspace(-1, 1, 100)\n",
    "train_labels = target_beta_1 * train_data + target_beta_0 + np.random.rand(train_data.shape[0]) * noise\n",
    "\n",
    "\n",
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying MNIST numbers with Neural Nets with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original', data_home='~/datasets/mnist')\n",
    "X, Y = mnist.data, mnist.target\n",
    "X = X / 255.0\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "numExamples = 2000\n",
    "test_data, test_labels = X[70000-numExamples:], Y[70000-numExamples:]\n",
    "train_data, train_labels = X[:numExamples], Y[:numExamples]\n",
    "numFeatures = train_data[1].size\n",
    "numClasses = 10\n",
    "numTrainExamples = train_data.shape[0]\n",
    "numTestExamples = test_data.shape[0]\n",
    "print('Features = %d' %(numFeatures))\n",
    "print('Train set = %d' %(numTrainExamples))\n",
    "print('Test set = %d' %(numTestExamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 5))\n",
    "plt.rc('image', cmap='binary', interpolation='none')\n",
    "\n",
    "for i in range(10):\n",
    "    for idx, l in enumerate(train_labels):\n",
    "        if i==l:\n",
    "            ax = plt.subplot(1, 10, i+1)\n",
    "            plt.setp(ax, xticks=(), yticks=())  \n",
    "            plt.imshow(train_data[idx].reshape(28, 28))\n",
    "            break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking ahead to working with neural networks, let's prepare one additional variation of the label data.  Let's make these labels, rather than each being an integer value from 0-9, be a set of 10 binary values, one for each class.  This is sometimes called a 1-of-n encoding, and it makes working with Neural Networks easier, as there will be one output node for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "original_Y = tf.constant(train_labels[:10])\n",
    "\n",
    "bin_Y = tf.one_hot(train_labels[:10], 10)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    res_original_Y, res_bin_Y = sess.run([original_Y, bin_Y])\n",
    "    print(res_bin_Y)\n",
    "    print()\n",
    "    print(res_original_Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start working in TensorFlow.  Before we jump to multi-layer neural networks though, let's train a logistic regression model to make certain we're using Tensorflow correctly. \n",
    "\n",
    "Recall from Josh's regression lecture the four key components: (1) parameters, (2) model, (3) cost function, and (4) objective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Parameters\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Constants\n",
    "trainY = tf.one_hot(train_labels, 10)\n",
    "testY = tf.one_hot(test_labels, 10)\n",
    "trainX = tf.constant(train_data, dtype = tf.float32)\n",
    "testX = tf.constant(test_data, dtype = tf.float32)\n",
    "\n",
    "# and Variables\n",
    "w = tf.get_variable('w', shape=[numFeatures, numClasses])\n",
    "#w = tf.Print(w, [w], 'weights are updated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two notes relevant at this point:\n",
    "\n",
    "First, logistic regression can be thought of as a neural network with no hidden layers. The output values are just the dot product of the inputs and the edge weights.\n",
    "\n",
    "Second, we have 10 classes. We can either train separate one vs all classifiers using sigmoid activation, which would be a hassle, or we can use the softmax activation, which is essentially a multi-class version of sigmoid. We'll use Theano's built-in implementation of softmax.\n",
    "\n",
    "$$x\\in \\mathbb{R}^n$$\n",
    "\n",
    "$$\\sigma(x)_j = \\frac{e^{x_j}}{\\sum_{i=0}^n e^{x_i}}$$\n",
    "\n",
    "$$\\sum_{j}\\sigma(x)_j=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Model\n",
    "\n",
    "def model(input_layer):\n",
    "    output_layer = tf.nn.softmax(tf.matmul(input_layer, w))\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use cross-entropy as a cost function.  Cross entropy only considers the error between the true class and the prediction, and not the errors for the false classes.  This tends to cause the network to converge faster.  We'll use Tensorflow's built-in cross entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Cost\n",
    "#?tf.losses.log_loss\n",
    "def cost_func(data, labels):\n",
    "    cc = tf.losses.log_loss(labels, model(data))\n",
    "    return  cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is minimize the cost, and to do that we'll use gradient descent.\n",
    "\n",
    "We'll use Tensorflow's built-in gradient function.  Exercise: Do you recall from Josh's lecture what the gradient is for beta in logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Ojbective (and solver)\n",
    "\n",
    "cc = cost_func(trainX, trainY)\n",
    "gd = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "step = gd.minimize(cc)\n",
    "test_preds = model(testX)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_vec = []\n",
    "    for i in range(1000):\n",
    "        _, cost, test__preds = sess.run([step, cc, test_preds])\n",
    "        cost_vec.append(cost)\n",
    "        if i%50 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test__preds, axis=1) == test_labels)))\n",
    "    \n",
    "plt.plot(cost_vec)  \n",
    "plt.xlabel('Number of Gradient Descent steps')\n",
    "plt.ylabel('Cost function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use batch gradient descent.\n",
    "\n",
    "Exercise: What are the differences between batch, stochastic, and mini-batch gradient descent?  What are the implications of each for working on large datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Parameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Constants\n",
    "testY = tf.one_hot(test_labels, numClasses)\n",
    "testX = tf.constant(test_data, dtype = tf.float32)\n",
    "\n",
    "# placeholders\n",
    "# In Tensorflow, placeholder are prepared to be filled with different python objects in the session.\n",
    "# Using these placeholder, we don't need to transform the python objects train_data and train_labels into \n",
    "# Tensorflow object. tf.placeholder does this for us.\n",
    "x_ = tf.placeholder(tf.float32, shape=[None, numFeatures], name='x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "\n",
    "# and Variables\n",
    "w = tf.get_variable('w', shape=[numFeatures, numClasses])\n",
    "\n",
    "\n",
    "# (2) Model\n",
    "def model(input_layer):\n",
    "    output_layer = tf.nn.softmax(tf.matmul(input_layer, w))\n",
    "    return output_layer\n",
    "\n",
    "# (3) Cost\n",
    "def cost_func(data, labels):\n",
    "    cc = tf.losses.log_loss(labels, model(data))\n",
    "    return  cc\n",
    "\n",
    "# (4) Ojbective (and solver)\n",
    "y_one_hot = tf.one_hot(y_, numClasses)\n",
    "cc = cost_func(x_, y_one_hot)\n",
    "gd = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "step = gd.minimize(cc)\n",
    "test_preds = model(testX)\n",
    "\n",
    "miniBatchSize = 10;\n",
    "num_samples = train_data.shape[0]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_vec = []\n",
    "    for i in range(50):\n",
    "        for start, end in zip(range(0, num_samples, miniBatchSize), range(miniBatchSize, num_samples, miniBatchSize)):\n",
    "            batch = train_data[start:end], train_labels[start:end]\n",
    "            # If we run step in the session, we specify x_ and y_ by using fed_dict as argument\n",
    "            _, cost, test__preds = sess.run([step, cc, test_preds], feed_dict={x_: batch[0], y_: batch[1]})\n",
    "        cost_vec.append(cost)\n",
    "        clear_output(wait=True)\n",
    "        print('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test__preds, axis=1) == test_labels)))\n",
    "        \n",
    "    # We find some correctly and wrongly predicted test samples    \n",
    "    count_correct = 0\n",
    "    count_wrong = 0\n",
    "    idx_correct = []\n",
    "    idx_wrong = []\n",
    "    predicted_as = []\n",
    "    for idx, p, l in zip(range(len(test_labels)), np.argmax(test__preds, axis=1), test_labels):\n",
    "        if p == l and count_correct < 3:\n",
    "            idx_correct.append(idx)\n",
    "            count_correct += 1\n",
    "        elif p != l and count_wrong < 3:\n",
    "            idx_wrong.append(idx)\n",
    "            predicted_as.append(p)\n",
    "            count_wrong += 1\n",
    "    \n",
    "plt.plot(cost_vec)  \n",
    "plt.xlabel('Number of Gradient Descent steps')\n",
    "plt.ylabel('Cost function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################  correctly predicted ##############\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.rc('image', cmap='binary', interpolation='none')\n",
    "\n",
    "print(\"Correctly predicted:\")\n",
    "for i, idx in enumerate(idx_correct):\n",
    "    ax = plt.subplot(1, count_correct, i+1)\n",
    "    plt.setp(ax, xticks=(), yticks=())  \n",
    "    plt.imshow(test_data[idx].reshape(28, 28))\n",
    "plt.show()\n",
    "\n",
    "################### wrongly predicted ##############\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.rc('image', cmap='binary', interpolation='none')\n",
    "\n",
    "print(\"Wrongly predicted:\")\n",
    "for i, idx in enumerate(idx_wrong):\n",
    "    print(\"Predicted as: \", predicted_as[i])\n",
    "    ax = plt.subplot(1, count_correct, i+1)\n",
    "    plt.setp(ax, xticks=(), yticks=())  \n",
    "    plt.imshow(test_data[idx].reshape(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
