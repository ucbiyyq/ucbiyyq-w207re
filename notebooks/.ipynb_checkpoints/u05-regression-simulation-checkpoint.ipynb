{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression via simulation; variable selection; regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a basic but important tool. In this notebook we'll look at it from a simulation point of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# regularization methos\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a couple of simulation functions: what do they do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions we will need for making the data\n",
    "def make_independent_data(dimension=3, n=1000):\n",
    "    '''\n",
    "    Makes independent data in \"dimension\" dimensions\n",
    "    '''\n",
    "    covar = np.diag([1 for i in range(dimension)])\n",
    "    dat_i = np.random.multivariate_normal([0 for i in range(dimension)], \n",
    "                                          covar, n)\n",
    "    \n",
    "    return dat_i, covar\n",
    "\n",
    "def make_correlated_data(dimension, n=1000):\n",
    "    '''\n",
    "    Makes dependent data in \"dimension\" dimensions\n",
    "    '''\n",
    "    \n",
    "    covar = np.zeros((dimension, dimension))\n",
    "    covar_vec = np.linspace(1,0,dimension)\n",
    "    for ii in range(dimension):\n",
    "        for jj in range(dimension):\n",
    "            covar[ii, jj] = covar_vec[np.abs(ii-jj)]\n",
    "        dat = np.random.multivariate_normal([0 for i in range(dimension)], covar, n)\n",
    "    return dat, covar\n",
    "\n",
    "\n",
    "x, covar = make_correlated_data(5, n=10)\n",
    "\n",
    "print(\"\\ncovariance matrix: \")\n",
    "print(covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between the two? I've added some correlation between the adjacent indices of variables in the second one, let's see what things look like with some plots.\n",
    "\n",
    "Try this with the correlated data, what do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_points = 100\n",
    "n_features = 5\n",
    "\n",
    "X, covar = make_independent_data(n_features, n_data_points)\n",
    "#X, covar = make_correlated_data(n_features, n_data_points)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "covar_plot = np.zeros((n_features,n_features))\n",
    "count = 0\n",
    "for ii in range(n_features):\n",
    "    for jj in range(n_features):\n",
    "        ax = plt.subplot(n_features, n_features, count + 1)\n",
    "        plt.setp(ax, xticks=(), yticks=())    \n",
    "        plt.scatter(X[:, ii], X[:, jj])  \n",
    "        ax.set_xlim([np.min(X), np.max(X)])\n",
    "        ax.set_ylim([np.min(X), np.max(X)])\n",
    "        count += 1\n",
    "        \n",
    "        covar_plot[ii,jj] = covar[n_features-1-ii,jj]\n",
    "        \n",
    "fig.suptitle('Pairwise Scatter Plot', fontsize=20)\n",
    "        \n",
    "        \n",
    "fig = plt.figure()\n",
    "heatmap = plt.pcolor(covar_plot, cmap=mp.cm.Blues)\n",
    "plt.axis('off')\n",
    "fig.suptitle('Heatmap of Covariance Matrix', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_set(n_data_points, \n",
    "                    n_features, \n",
    "                    n_noise_features, \n",
    "                    signal_strength, \n",
    "                    n_train_points):\n",
    "    \"\"\"Create a complete data set for a regression problem\n",
    "\n",
    "    Arguments:\n",
    "    n_data_points: integer number of data points\n",
    "    n_features: integer number of features that have nonzero coefficients\n",
    "    n_noise_features: integer number of features that have zero coefficients\n",
    "    signal_strength: mean value of nonzero coefficients\n",
    "    n_train_points: number of data points to place in training set from the total n_data_points\n",
    "    \"\"\"\n",
    "    \n",
    "    # create true set of coefficients\n",
    "    true_linear_model = np.concatenate((np.random.normal(size=n_features, scale=0.5) + signal_strength, \n",
    "                                        np.zeros(n_noise_features)))\n",
    "\n",
    "    # make things the right shape\n",
    "    true_linear_model = np.reshape(np.array(true_linear_model), \n",
    "                                   newshape=(n_features + n_noise_features, 1))\n",
    "\n",
    "    # make the data now using make_correlated_data\n",
    "    X, _ = make_correlated_data(n_features + n_noise_features, n_data_points)\n",
    "\n",
    "    # use the features and the coefficients to make the data\n",
    "    Y_signal = np.dot(X, true_linear_model)\n",
    "    Y_noise = np.reshape(np.random.normal(size=n_data_points), newshape=(n_data_points, 1))\n",
    "    Y = np.add(Y_signal, Y_noise)\n",
    "\n",
    "    # make Y the right shape\n",
    "    Y = Y.flatten()\n",
    "\n",
    "    # train, test split\n",
    "    train_data, train_labels = X[:n_train_points], Y[:n_train_points]\n",
    "    test_data, test_labels = X[n_train_points:], Y[n_train_points:]\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels, true_linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run some simulations: what variables does linear regression pick up?\n",
    "\n",
    "Is this a harder problem with correlated data?\n",
    "\n",
    "What are some ways we can reduce estimation error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels, true_linear_model = create_data_set(500, 5, 2, 1, 350)\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(train_data, train_labels)\n",
    "\n",
    "lm_coef = lm.coef_\n",
    "\n",
    "lm_test_mse = ((lm.predict(test_data) - test_labels) ** 2).mean()\n",
    "\n",
    "print(\"Linear model test MSE: \", lm_test_mse)\n",
    "\n",
    "# plot estimated coefficients versus the actual coefficients\n",
    "fig = plt.figure()\n",
    "plt.plot([min(true_linear_model),max(true_linear_model)],[min(true_linear_model),max(true_linear_model)],'r--')\n",
    "plt.scatter(true_linear_model, lm_coef)\n",
    "fig.suptitle('Estimated Coefficients vs Actual', fontsize=20)\n",
    "plt.xlabel('True Coefs')\n",
    "plt.ylabel('Est Coefs') \n",
    "\n",
    "# these are the indices of nonzero entries of the coefficient vector\n",
    "print('index of nonzero linear regression variables: \\n', np.where(lm_coef != 0))\n",
    "\n",
    "# what is wrong with this measure?\n",
    "estimation_error = ((true_linear_model - lm_coef) ** 2).mean()\n",
    "\n",
    "print(\"Estimation error for parameters: \", estimation_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that is going wrong here: we are assigning non-zero vlaues to coefficients with truly zero coefficients.\n",
    "\n",
    "Now, we can try implementing stepwise regression, which is a simple greedy algorithm for selecting a sub model. Here's a plan:\n",
    "\n",
    "1. Start with an empty model (no variables used)\n",
    "2. For each feature not included:\n",
    "a. Fit a model with that feature included, record the prediction performance\n",
    "3. Add the feature that did the best in 2a to the set\n",
    "4. Repeat until all features are in the model\n",
    "\n",
    "How could we choose how to stop?\n",
    "\n",
    "How can we see if it is reasonable to stop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100) # \"simulation\"\n",
    "\n",
    "## If the number of data points are larger, the not selected coefficients are the 0 \"noice_features\"\n",
    "train_data, train_labels, test_data, test_labels, true_linear_model = create_data_set(1000, 5, 3, 1, 60)\n",
    "\n",
    "invar = []\n",
    "outvar = list(range(train_data.shape[1]))\n",
    "\n",
    "# begin with the null model\n",
    "intercept = test_labels.mean()\n",
    "error = ((test_labels - intercept) ** 2).mean()\n",
    "\n",
    "errorseq = [error]\n",
    "\n",
    "##################\n",
    "# implement the algorithm here\n",
    "\n",
    "for jj in range(train_data.shape[1]):\n",
    "    varscores = []\n",
    "    print('round ' + str(jj))\n",
    "    for idx, ovar in enumerate(outvar):\n",
    "        \n",
    "        ## YOUR CODE HERE: select best feature\n",
    "       \n",
    "        \n",
    "    min_score = np.argmin(np.array(varscores))\n",
    "    errorseq.append(varscores[min_score]) # store the score of the best variable\n",
    "    invar.append(outvar[min_score]) # choose the best variable in this step\n",
    "    outvar.pop(min_score)\n",
    "    print('Variables included: ', invar, '\\n')\n",
    "\n",
    "###################    \n",
    "\n",
    "# the final decisions\n",
    "adj_differences = np.array(errorseq[1:]) - np.array(errorseq[:-1])\n",
    "cut_process = (np.where(adj_differences > 0)[0][0])\n",
    "\n",
    "# results; diagnostics\n",
    "print(\"Sequence of test set errors:\\n\", errorseq, '\\n')\n",
    "print(\"Differences btw steps:\\n\", adj_differences, '\\n')\n",
    "print(\"Final set of variables:\\n\", invar[:cut_process], '\\n')\n",
    "print(\"Selected coefficients; true values:\\n\", true_linear_model[invar[:cut_process]][:,0], '\\n')\n",
    "print('Selected coefficients; estimated values:\\n', lm.coef_[invar[:cut_process]], '\\n')\n",
    "print('Not selected coefficients; true values:\\n', true_linear_model[invar[cut_process:]][:,0], '\\n')\n",
    "print('Not selected coefficients; estimated values:\\n', lm.coef_[invar[cut_process:]], '\\n')\n",
    "fig = plt.figure()\n",
    "plt.scatter(range(len(errorseq))[1:], errorseq[1:])\n",
    "fig.suptitle('Test Set Errors by Step', fontsize=20)\n",
    "plt.xlabel('Step Number')\n",
    "plt.ylabel('Test Set Error') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this the only way to do variable selection? Turns out there is a way to cast this entire task as a single optimization problem, called *regularization*.\n",
    "\n",
    "**Regularization in general**\n",
    "\n",
    "Typical learning objectives or cost functions simply optimize the fit. It looks like (here beta are some parameters that define the model, say the coefficients in linear regression):\n",
    "\n",
    "$Cost(\\beta) = fit(\\beta) = \\sum (prediction - label)^2$\n",
    "\n",
    "And we do:\n",
    "\n",
    "$\\hat\\beta = argmin(Cost(\\beta))$\n",
    "\n",
    "So, we choose the beta that minimizes the cost, that is, provides the best fit to the training data.\n",
    "\n",
    "Regularization simply tweaks this by:\n",
    "\n",
    "$RegularizedCost(\\beta) = fit(\\beta) + \\alpha * penalty(\\beta)$\n",
    "\n",
    "And, as above:\n",
    "\n",
    "$regularized\\ \\hat\\beta = argmin(RegularizedCost(\\beta))$\n",
    "\n",
    "So, what is the penalty? It is a way to define the complexity of $\\beta$. We'll see how complexity is defined below. But, in general, as the fit gets better, the complexity becomes higher, and so the objective nicely formalizes this tradeoff. $\\alpha$ is a constant (we'll have to pick it) that defines the strength of this tradeoff.\n",
    "\n",
    "We can define complexity in a couple of ways: (1) large values of coefficients are one indication; (2) using many dimensions (non-zero coefficients) are another indication. Regularization can caputre both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 regularization: shrinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the first type of regularization, L2 regularization. It takes the form:\n",
    "\n",
    "$Cost(\\beta) = fit(\\beta) + \\alpha * \\sum_j\\beta_j^2$\n",
    "\n",
    "Some useful docs for the following:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(train_data, train_labels)\n",
    "\n",
    "lm_test_mse = ((lm.predict(test_data) - test_labels) ** 2).mean()\n",
    "\n",
    "rm = Ridge(alpha=1000, normalize=False)\n",
    "rm.fit(train_data, train_labels)\n",
    "\n",
    "rm_test_mse = ((rm.predict(test_data) - test_labels) ** 2).mean()\n",
    "\n",
    "print(\"Linear model test MSE:     \", lm_test_mse)\n",
    "print(\"Ridge regression test MSE: \", rm_test_mse)\n",
    "print(\"Linear regression does worse on the test set: \", lm_test_mse > rm_test_mse)\n",
    "\n",
    "# how can we choose a the best paramater?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what happens to the coefficients as alpha changes. Here's an investigation for a single coefficient. What happens to other coefficients? How is this reducing complexity? You can also try out making a *regularization path plot*, see:\n",
    "\n",
    "http://fa.bianp.net/blog/2011/ridge-regression-path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha_vec = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "coef_idx = 6\n",
    "coef_values = np.zeros(len(alpha_vec))\n",
    "lm_coef_line = np.zeros(len(alpha_vec))\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(train_data, train_labels)\n",
    "lm_coef = lm.coef_[coef_idx]\n",
    "lm_test_mse = ((lm.predict(test_data) - test_labels) ** 2).mean()\n",
    "\n",
    "for aa_idx, aa in enumerate(alpha_vec):\n",
    "    rm = Ridge(alpha=aa, normalize=True)\n",
    "    rm.fit(train_data, train_labels)\n",
    "    rm_coef = rm.coef_[coef_idx]\n",
    "    \n",
    "    coef_values[aa_idx] = rm_coef\n",
    "    lm_coef_line[aa_idx] = lm_coef\n",
    "    \n",
    "    rm_test_mse = ((rm.predict(test_data) - test_labels) ** 2).mean()\n",
    "    print(\"Alpha is\", aa)\n",
    "    print(rm_test_mse)\n",
    "    print(\"Linear regression does worse on the test set: \", lm_test_mse > rm_test_mse)\n",
    "    \n",
    "plt.plot(np.log(alpha_vec), lm_coef_line * 0)    \n",
    "plt.plot(np.log(alpha_vec), coef_values)\n",
    "plt.plot(np.log(alpha_vec), lm_coef_line)\n",
    "plt.scatter(np.log(alpha_vec), coef_values)\n",
    "plt.title('Ridge regularization path\\n for coefficient ' + str(coef_idx) + \n",
    "          '; lm value (green straight line): ' + str(lm_coef))\n",
    "plt.ylabel('Coefficient Values') \n",
    "plt.xlabel('log(Alpha)') \n",
    "plt.show()\n",
    "\n",
    "print(coef_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularization: the lasso; shrinkage and selection\n",
    "\n",
    "Let's explore another type of regularization, L1 regularization. It takes the form:\n",
    "\n",
    "$Cost(\\beta) = fit(\\beta) + \\alpha * \\sum_j|\\beta_j|$\n",
    "\n",
    "Some useful docs for the following:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(train_data, train_labels)\n",
    "\n",
    "lm_test_mse = ((lm.predict(test_data) - test_labels) ** 2).mean()\n",
    "\n",
    "# try playing with the alpha\n",
    "las = Lasso(alpha=0.01, normalize=True, max_iter=100)\n",
    "las.fit(train_data, train_labels)\n",
    "\n",
    "las_test_mse = ((las.predict(test_data) - test_labels) ** 2).mean()\n",
    "\n",
    "print(\"Linear model test MSE: \", lm_test_mse)\n",
    "print(\"Lasso regression test MSE: \", las_test_mse)\n",
    "\n",
    "print(\"Linear regression does worse on the test set: \", lm_test_mse > las_test_mse)\n",
    "\n",
    "print(true_linear_model)\n",
    "print(lm.coef_)\n",
    "print(las.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_vec = [0.00001, 0.0001, 0.001, 0.01, 0.1,1, 10, 100]\n",
    "coef_idx = 5\n",
    "coef_values = np.zeros(len(alpha_vec))\n",
    "lm_coef_line = np.zeros(len(alpha_vec))\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(train_data, train_labels)\n",
    "lm_coef = lm.coef_[coef_idx]\n",
    "lm_test_mse = ((lm.predict(test_data) - test_labels) ** 2).mean()\n",
    "\n",
    "for aa_idx, aa in enumerate(alpha_vec):\n",
    "\n",
    "    las = Lasso(alpha=aa, normalize=True, max_iter=1000)\n",
    "    las.fit(train_data, train_labels)\n",
    "    las_coef = las.coef_[coef_idx]\n",
    " \n",
    "    coef_values[aa_idx] = las_coef\n",
    "    lm_coef_line[aa_idx] = lm_coef\n",
    "\n",
    "    las_test_mse = ((las.predict(test_data) - test_labels) ** 2).mean()\n",
    "    print(\"Alpha is \", aa)\n",
    "    print(las_test_mse)\n",
    "    print(\"Linear regression does worse on the test set: \", lm_test_mse > las_test_mse)\n",
    "    \n",
    "print(coef_values)\n",
    "\n",
    "plt.plot(np.log(alpha_vec), lm_coef_line * 0)\n",
    "plt.plot(np.log(alpha_vec), coef_values)\n",
    "plt.plot(np.log(alpha_vec), lm_coef_line)\n",
    "plt.scatter(np.log(alpha_vec), coef_values)\n",
    "plt.title('Lasso regularization path\\n for coefficient ' + str(coef_idx) + \n",
    "          '; lm value (green straight line): ' + str(lm_coef))\n",
    "plt.ylabel('Coefficient Values') \n",
    "plt.xlabel('log(Alpha)') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
