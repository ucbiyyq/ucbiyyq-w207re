{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and test a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data, which is included in sklearn.\n",
    "iris = load_iris()\n",
    "print('Iris target names:', iris.target_names)\n",
    "print('Iris feature names:', iris.feature_names)\n",
    "X, Y = iris.data, iris.target\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "# Split into train and test.\n",
    "train_data, train_labels = X[:100], Y[:100]\n",
    "test_data, test_labels = X[100:], Y[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris feature values are real valued -- measurements in centimeters. Let's look at histograms of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure and set the figsize argument so we get square-ish plots of the 4 features.\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "# Iterate over the features, creating a subplot with a histogram for each one.\n",
    "for feature in range(train_data.shape[1]):\n",
    "    plt.subplot(1, 4, feature+1)\n",
    "    plt.hist(train_data[:,feature], 20)\n",
    "    plt.title(iris.feature_names[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things simple, let's binarize these feature values. That is, we'll treat each measurement as either \"short\" or \"long\". I'm just going to choose a threshold for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that applies a threshold to turn real valued iris features into 0/1 features.\n",
    "# 0 will mean \"short\" and 1 will mean \"long\".\n",
    "def binarize_iris(data, thresholds=[6.0, 3.0, 2.5, 1.0]):\n",
    "    # Initialize a new feature array with the same shape as the original data.\n",
    "    binarized_data = np.zeros(data.shape)\n",
    "\n",
    "    # Apply a threshold  to each feature.\n",
    "    for feature in range(data.shape[1]):\n",
    "        binarized_data[:,feature] = data[:,feature] > thresholds[feature]\n",
    "    return binarized_data\n",
    "\n",
    "# Create new binarized training and test data\n",
    "binarized_train_data = binarize_iris(train_data)\n",
    "binarized_test_data = binarize_iris(test_data)\n",
    "\n",
    "print(train_data[:10, ])\n",
    "print(binarized_train_data[:10, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that Naive Bayes assumes conditional independence of features. With $Y$ the set of labels and $X$ the set of features ($y$ is a specific label and $x$ is a specific feature), Naive Bayes gives the probability of a label $y$ given input features $X$ as:\n",
    "\n",
    "$ \\displaystyle P(y|X) \\approx \n",
    "  \\frac{P(y) \\prod _{x\\in X} P(x|y)}{P(X)}\n",
    "  =  \\frac { P(y) \\prod_{x \\in X} P(x|y) }\n",
    "        { \\sum_{y \\in Y} P(y) \\prod_{x \\in X} P(x|y) }\n",
    "$\n",
    "\n",
    "Let's estimate some of these probabilities using maximum likelihood, which is just a matter of counting and normalizing. We'll start with the prior probability of the label $P(y)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize counters for all labels to zero.\n",
    "label_counts = [0 for i in iris.target_names]\n",
    "\n",
    "# Iterate over labels in the training data and update counts.\n",
    "for label in train_labels:\n",
    "    label_counts[label] += 1\n",
    "\n",
    "# Normalize counts to get a probability distribution.\n",
    "total = sum(label_counts)\n",
    "label_probs = [1.0 * count / total for count in label_counts]\n",
    "for (prob, name) in zip(label_probs, iris.target_names):\n",
    "    print('%15s : %.2f' %(name, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's estimate $P(X|Y)$, that is the probability of each feature given each label. Remember that we can get the conditional probability from the joint distribution:\n",
    "\n",
    "$\\displaystyle P(X|Y) = \\frac{ P(X,Y) } { P(Y) } \\approx \\frac{ \\textrm{Count}(X,Y) } { \\textrm{Count}(Y) }$\n",
    "\n",
    "Let's think carefully about the size of the count matrix we need to collect. There are 3 labels $y_0$, $y_1$, and $y_2$ and 4 features $x_0$, $x_1$, $x_2$, and $x_3$. Each feature has 2 possible values, 0 or 1. So there are actually $4 \\times 2 \\times 3=24$ probabilities we need to estimate: \n",
    "\n",
    "$P(x_0=0, Y=y_0)$\n",
    "\n",
    "$P(x_0=1, Y=y_0)$\n",
    "\n",
    "$P(x_1=0, Y=y_0)$\n",
    "\n",
    "$P(x_1=1, Y=y_0)$\n",
    "\n",
    "...\n",
    "\n",
    "However, we already estimated (above) the probability of each label. And, we know that each feature value is either 0 or 1. So, for example,\n",
    "\n",
    "$P(x_0=0, Y=\\textrm{setosa}) + P(x_0=1, Y=\\textrm{setosa}) = P(Y=\\textrm{setosa}) \\approx 0.31$.\n",
    "\n",
    "As a result, we can just estimate probabilities for one of the feature values, say, $x_i = 0$. This requires a $4 \\times 3$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a matrix for joint counts of feature=0 and label.\n",
    "feature0_and_label_counts = np.zeros([len(iris.feature_names), len(iris.target_names)])\n",
    "\n",
    "# Just to check our work, let's also keep track of joint counts of feature=1 and label.\n",
    "feature1_and_label_counts = np.zeros([len(iris.feature_names), len(iris.target_names)])\n",
    "\n",
    "for i in range(binarized_train_data.shape[0]):\n",
    "    # Pick up one training example at a time: a label and a feature vector.\n",
    "            \n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Let's look at the counts.\n",
    "print('Feature = 0 and label:\\n', feature0_and_label_counts)\n",
    "print('\\nFeature = 1 and label:\\n', feature1_and_label_counts)\n",
    "\n",
    "# As a sanity check, what should the total sum of all counts be?\n",
    "# We have 100 training examples, each with 4 features. So we should have counted 400 things.\n",
    "total_sum = feature0_and_label_counts.sum() + feature1_and_label_counts.sum()\n",
    "print('\\nTotal count:', total_sum)\n",
    "\n",
    "# As another sanity check, the label probabilities should be equal to the normalized feature counts for each label.\n",
    "print('Label probabilities:', (feature0_and_label_counts.sum(0) + feature1_and_label_counts.sum(0)) / total_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to normalize the joint counts to get probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize new matrices to hold conditional probabilities.\n",
    "feature0_given_label = np.zeros(feature0_and_label_counts.shape)\n",
    "feature1_given_label = np.zeros(feature1_and_label_counts.shape)\n",
    "\n",
    "# P(feature|label) = P(feature, label) / P(label) =~ count(feature, label) / count(label).\n",
    "# Note that we could do this normalization more efficiently with array operations, but for the sake of clarity,\n",
    "# let's iterate over each label and each feature.\n",
    "for label in range(feature0_and_label_counts.shape[1]):\n",
    "    for feature in range(feature0_and_label_counts.shape[0]):\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "       \n",
    "        \n",
    "# Here's our estimated conditional probability table.\n",
    "print('Estimated values of P(feature=0|label):\\n', feature0_given_label)\n",
    "\n",
    "print('Estimated values of P(feature=1|label):\\n', feature1_given_label)\n",
    "\n",
    "# As a sanity check, which probabilities should sum to 1?\n",
    "print('\\nCheck that P(feature=0|label) + P(feature=1|label) = 1\\n',feature0_given_label + feature1_given_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the pieces, let's try making a prediction for the first test example. It looks like this is a setosa (label 0) example with all small measurements -- all the feature values are 0.\n",
    "\n",
    "We start by assuming the prior distribution, which has a slight preference for virginica, followed by versicolor. Of course, these estimates come from our training data, which might not be a representative sample. In practice, we may prefer to use a uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the feature vector look like? And what's the true label?\n",
    "index = 4\n",
    "print('Feature vector:', binarized_test_data[index])\n",
    "print('Label:', test_labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate for each label the probability that the test example with its specific features has this label.\n",
    "\n",
    "\\begin{align}\n",
    "P(Y=s|X) &\\approx P(x_0=0|Y=s)P(x_1=0|Y=s)P(x_2=0|Y=s)P(x_3=0|Y=s)P(Y=s)\\\\\n",
    "P(Y=ve|X) &\\approx P(x_0=0|Y=ve)P(x_1=0|Y=ve)P(x_2=0|Y=ve)P(x_3=0|Y=ve)P(Y=ve)\\\\\n",
    "P(Y=vi|X) &\\approx P(x_0=0|Y=vi)P(x_1=0|Y=vi)P(x_2=0|Y=vi)P(x_3=0|Y=vi)P(Y=vi)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's include the first feature. We use feature0_given_label since the feature value is 0.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "predictions = feature1_given_label[0]*feature0_given_label[1]*feature1_given_label[2]*feature1_given_label[3]*label_probs\n",
    "\n",
    "print(predictions)\n",
    "# We could wait until we've multiplied by all the feature probabilities, \n",
    "# but there's no harm in normalizing after each update.\n",
    "predictions /= predictions.sum()\n",
    "print('After normalizing:', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also predicts label 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?\n",
    "\n",
    "Well, it looks like Naive Bayes came up with the right answer. But it seems overconfident!\n",
    "\n",
    "Let's look again at our conditional probability estimates for the features. Notice that there are a bunch of zero probabilities. This is bad because as soon as we multiply anything by zero, we're guaranteed that our final estimate will be zero. This is an overly harsh penalty for an observation that simply never occurred in our training data. Surely there's some possibility, even if very small, that there could exist a setosa with a long sepal.\n",
    "\n",
    "This is where smoothing comes in. The maximum likelihood estimate is only optimal in the case where we have infinite training data. When we have less than that, we need to temper maximum likelihood by reserving some small probability for unseen events. The simplest way to do this is with Laplace smoothing -- rather than starting with a count of 0 for each joint (feature, label) observation, we start with a count of $\\alpha$.\n",
    "\n",
    "$\\displaystyle P(X|Y) = \\frac{ P(X,Y) } { P(Y) } \\approx \\frac{ \\textrm{Count}(X,Y) + \\alpha } { \\textrm{Count}(Y)  + \\alpha|X|} \\stackrel{\\text{here}}{=} \\frac{ \\textrm{Count}(X,Y) + \\alpha } { \\textrm{Count}(Y)  +  \\alpha 2 }$\n",
    "\n",
    "Let's package training and inference into a class, modeled after sklearn's BernoulliNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    # Initialize an instance of the class.\n",
    "    def __init__(self, alpha=0.0):\n",
    "        self.alpha = alpha     # additive (Laplace) smoothing parameter\n",
    "        self.priors = None     # estimated by fit()\n",
    "        self.probs = None      # estimated by fit()\n",
    "        self.num_labels = 0    # set by fit()\n",
    "        self.num_features = 0  # set by fit()\n",
    "        \n",
    "    def fit(self, train_data, train_labels):\n",
    "        # Store number of labels, number of features, and number training examples.\n",
    "        self.num_labels = len(np.unique(train_labels))\n",
    "        self.num_features = train_data.shape[1]\n",
    "        self.num_examples = train_data.shape[0]\n",
    "        \n",
    "        # Initialize an array of label counts. Each label gets a smoothed count of 2*alpha because\n",
    "        # each feature value (0 and 1) gets an extra count of alpha.\n",
    "        label_counts = np.ones(self.num_labels) * self.alpha * 2\n",
    "\n",
    "        # Initialize an array of (feature=1, label) counts to alpha.\n",
    "        feature0_and_label_counts = np.ones([self.num_features, self.num_labels]) * self.alpha\n",
    "    \n",
    "        # Count features with value == 1.\n",
    "        for i in range(self.num_examples):\n",
    "            label = train_labels[i]\n",
    "            label_counts[label] += 1\n",
    "            for feature_index, feature_value in enumerate(train_data[i]):\n",
    "                feature0_and_label_counts[feature_index][label] += (feature_value == 1)\n",
    "\n",
    "        # Normalize to get probabilities P(feature=1|label).\n",
    "        self.probs = feature0_and_label_counts / label_counts\n",
    "        \n",
    "        # Normalize label counts to get prior probabilities P(label).\n",
    "        self.priors = label_counts / label_counts.sum()\n",
    "\n",
    "    # Make predictions for each test example and return results.\n",
    "    def predict(self, test_data):\n",
    "        results = []\n",
    "        for item in test_data:\n",
    "            results.append(self._predict_item(item))\n",
    "        return np.array(results)\n",
    "    \n",
    "    # Private function for making a single prediction.\n",
    "    def _predict_item(self, item):\n",
    "        # Make a copy of the prior probabilities.\n",
    "        predictions = self.priors.copy()\n",
    "        \n",
    "        # Multiply by each conditional feature probability.\n",
    "        for (index, value) in enumerate(item):\n",
    "            feature_probs = self.probs[index]\n",
    "            if not value: feature_probs = 1 - feature_probs\n",
    "            predictions *= feature_probs\n",
    "\n",
    "        # Normalize and return the label that gives the largest probability.\n",
    "        predictions /= predictions.sum()\n",
    "        return predictions.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare our implementation with the sklearn implementation. Do the predictions agree? What about the estimated parameters? Try changing alpha from 0 to 1.\n",
    "\n",
    "Note: I think there might be a bug in the sklearn code. What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "nb = NaiveBayes(alpha=alpha)\n",
    "nb.fit(binarized_train_data, train_labels)\n",
    "\n",
    "# Compute accuracy on the test data.\n",
    "preds = nb.predict(binarized_test_data)\n",
    "correct, total = 0, 0\n",
    "for pred, label in zip(preds, test_labels):\n",
    "    if pred == label: correct += 1\n",
    "    total += 1\n",
    "print('With alpha = %.2f' %alpha)\n",
    "print('[OUR implementation] total: %3d  correct: %3d  accuracy: %3.2f' %(total, correct, 1.0*correct/total))\n",
    "\n",
    "# Compare to sklearn's implementation.\n",
    "clf = BernoulliNB(alpha=alpha, fit_prior=False)\n",
    "clf.fit(binarized_train_data, train_labels)\n",
    "print('sklearn accuracy: %3.2f' %clf.score(binarized_test_data, test_labels))\n",
    "\n",
    "print('\\nOur feature probabilities\\n', nb.probs)\n",
    "print('\\nsklearn feature probabilities\\n', np.exp(clf.feature_log_prob_).T)\n",
    "\n",
    "print('\\nOur prior probabilities\\n', nb.priors)\n",
    "print('\\nsklearn prior probabilities\\n', np.exp(clf.class_log_prior_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
